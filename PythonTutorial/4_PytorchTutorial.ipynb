{"cells":[{"cell_type":"markdown","metadata":{"id":"V2c0k_OY4rrl"},"source":["# PyTorch Tutorial / Quickstart\n","\n","Large overlap with [PyTorch's 60min Blitz tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html?highlight=minute), tailored to image processing \n","and building your own networks.\n","\n","Overview:\n","- Define your **data**\n","- Construct your **network**\n","- **Train**, validate, test\n","\n","- [Enabling Google Cloud Platform](https://research.google.com/colaboratory/marketplace.html)"]},{"cell_type":"markdown","metadata":{"id":"d4eMgnQLevP3"},"source":["## TENSORS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25571,"status":"ok","timestamp":1681906530657,"user":{"displayName":"Alaqian Zafar","userId":"09077679626904495344"},"user_tz":240},"id":"i8tIWN_klJu7","outputId":"71360bd8-5f06-498f-ccb5-128b733f6a42"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"el3TJQW8dZ9e"},"outputs":[],"source":["import numpy as np\n","import glob              # easy file searching\n","from PIL import Image    # image loading\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torchvision as tv # data augmentation/loading utilities"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Dg0kS_rdbrz"},"outputs":[],"source":["# can also wrap a numpy array\n","a = torch.tensor([0, 5])\n","b = torch.tensor([[1, 5],[2,10]])\n","# broadcasting over dim 0\n","c = a + b"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6mvpRu2FdhuT"},"outputs":[],"source":["# show c\n","c"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvAQUplxdrBh"},"outputs":[],"source":["c.requires_grad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zw3YS_Bsd4qN"},"outputs":[],"source":["img = np.array(Image.open(\"/content/drive/MyDrive/CBSD432/100007.jpg\").convert('L'))\n","print(img.shape)\n","print(img.min(), img.max())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBZqNy_gejzJ"},"outputs":[],"source":["imgt = torch.tensor(img).float() / 255.0\n","imgt.shape\n","print(imgt.min(), imgt.max())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fSUjnqBIPCaj"},"outputs":[],"source":["plt.imshow(img)"]},{"cell_type":"markdown","metadata":{"id":"TPYQ4q0lpCL1"},"source":["## Example Problem: DeNoising\n","\n","$$\n","y = x + v, \\quad v \\sim \\mathcal{N}(0,\\sigma I)\n","$$\n","\n","Our noisy image $y$ is modeled as our ground-truth image $x$ that has been contaminated by additive Gaussian white noise (AWGN). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vpy-UusDMP_u"},"outputs":[],"source":["def awgn(x, sigma):\n","  return x + sigma*torch.randn_like(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fD1hIC_-PI6H"},"outputs":[],"source":["y = awgn(imgt, 0.1)\n","print(y.shape)\n","plt.imshow(y)"]},{"cell_type":"markdown","metadata":{"id":"CpVIPoiJmjAK"},"source":["## DEFINING A NETWORK\n","\n","We're going to implement a Denoising neural network via the [Convolutional Dictionary Learning Network](https://nikopj.github.io/projects/dcdl/) (CDLNet) architecture. \n","\n","\n","$$\n","\\hat{x} = Dz^{(K)}\n","$$\n","\n","$$\n","z^{(k+1)} =  \\mathrm{ST}_{\\tau^{(k)}}\\left( z^{(k)} - A^{(k)}(B^{(k)}z^{(k)} - y)\\right),\\quad k=0,1,\\dots,K-1\n","$$\n","\n","$$\n","\\mathrm{ST}_{\\tau}(x) = \\mathrm{sign}(x) \\mathrm{ReLU}(\\lvert x \\rvert - \\tau)\n","$$\n","\n","where $k$ denotes the layer of the network, and $A,B,D$ are learned convolution operators."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AuBV0OB6rnaz"},"outputs":[],"source":["# soft-thresholding\n","def ST(x,t):\n","  return torch.sign(x)*F.relu(x.abs()-t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G5Koa73esBab"},"outputs":[],"source":["x = torch.linspace(-1, 1, 101)\n","y = ST(x, 0.25)\n","plt.plot(x, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xIxl2jzBtUry"},"outputs":[],"source":["x = torch.randn(5, requires_grad=True)\n","print(x)\n","y = ST(x,1)\n","print(y)\n","y.backward(gradient=torch.ones_like(x))\n","x.grad"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yG1k6a64mtqm"},"outputs":[],"source":["class CDLNet(nn.Module):\n","  def __init__(self, K, nf, ks):\n","    # K: layers\n","    # nf: number of hidden features / subbands\n","    # ks: kernel-size (considering odd filter sizes only, for ease of same-padding)\n","    # 4D tensors: batch x channel x height x width\n","    super().__init__()\n","\n","    self.A = nn.ModuleList([nn.Conv2d(1, nf, ks, padding=(ks-1)//2, bias=False) for _ in range(K)])\n","    self.B = nn.ModuleList([nn.Conv2d(nf, 1, ks, padding=(ks-1)//2, bias=False) for _ in range(K)])\n","    self.D = nn.Conv2d(nf, 1, ks, padding=(ks-1)//2, bias=False)\n","    self.tau = nn.ParameterList([nn.Parameter(torch.zeros(1,nf,1,1)) for _ in range(K)])\n","\n","    self.K = K\n","    self.nf = nf \n","    self.ks = ks\n","\n","  def forward(self, y):\n","    # pre-process input via mean-subtraction\n","    mu = y.mean(dim=(1,2), keepdim=True)\n","    y  = y - mu\n","\n","    # initialize hidden state as zeros\n","    B, C, M, N = y.shape\n","    z = torch.zeros((B,self.nf,M,N), device=y.device)\n","\n","    # run iterations\n","    for k in range(self.K):\n","      z = ST(z - self.A[k](self.B[k](z) - y), self.tau[k])\n","    \n","    # synthesize output from hidden state\n","    xhat = self.D(z)\n","\n","    # post-process with mean-addition\n","    return xhat + mu \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yhKdEbUbu6d3"},"outputs":[],"source":["net = CDLNet(10, 8, 5)\n","imgt = torch.tensor(img, dtype=torch.float)[None,None,:,:] / 255\n","print(imgt.shape, imgt.dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"toD3lD-svzjX"},"outputs":[],"source":["out = net(imgt)\n","out.shape\n","\n","plt.imshow(out[0,0].detach())\n","out.min()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_X__X92TzS3t"},"outputs":[],"source":["P = net.parameters()\n","for p in net.parameters():\n","  print(p.shape)"]},{"cell_type":"markdown","metadata":{"id":"e0O3jsZqmvHm"},"source":["## DATASETS AND DATALOADERS\n","\n","We're going to define our own dataset by overriding the base-class defined in PyTorch. See [the Dataset docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) for more details.\n","\n","We'll also use tools from TorchVision for augmenting the data. Further augmentation functions can be found in [the docs](https://pytorch.org/vision/stable/transforms.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBtc87l5nDOj"},"outputs":[],"source":["\n","class MyImageFolder(torch.utils.data.Dataset):\n","  def __init__(self, root, crop_size):\n","    super().__init__()\n","    # get filenames\n","    self.files = glob.glob(root+\"/*\")\n","\n","    # load grayscale images\n","    self.images = [Image.open(fn).convert('L') for fn in self.files]\n","\n","    # define your transform\n","    self.transform = tv.transforms.Compose([tv.transforms.RandomCrop(crop_size), tv.transforms.ToTensor()])\n","\n","  def __len__(self):\n","    return len(self.files)\n","\n","  def __getitem__(self, i):\n","    return self.transform(self.images[i])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGe3R7Aa2GoP"},"outputs":[],"source":["ds = MyImageFolder(\"/content/drive/MyDrive/CBSD432\", 64)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lkzn6OTzMqCU"},"outputs":[],"source":["out = ds[np.random.randint(len(ds))]\n","out.shape\n","\n","plt.imshow(out[0])\n","out.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8BmQJIB5IGI"},"outputs":[],"source":["# dataloader\n","\n","dl = torch.utils.data.DataLoader(ds, batch_size=2, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZ0uZL-IG06J"},"outputs":[],"source":["# checkpointing\n","\n","def save_ckpt(fn, network, optimizer):\n","  torch.save({'net': network.state_dict(), 'opt': opt.state_dict()}, fn)\n","\n","def load_ckpt(fn, network, optimizer):\n","  ckpt = torch.load(fn)\n","  network.load_state_dict(ckpt['net'])\n","  optimizer.load_state_dict(ckpt['opt'])\n","  return network, optimizer"]},{"cell_type":"markdown","metadata":{"id":"7DI0FXecm4_L"},"source":["## TRAINING "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWvQvw0inCnM"},"outputs":[],"source":["from tqdm.autonotebook import tqdm\n","import time\n","\n","SIGMA = 20/255 # noise-level\n","\n","if torch.cuda.device_count() > 0:\n","  device = torch.device(\"cuda:0\")\n","else:\n","  device = torch.device(\"cpu\")\n","\n","net = net.to(device)\n","opt = torch.optim.Adam(net.parameters(), lr=1e-3)\n","\n","avg_loss = 0\n","progress_bar = tqdm(dl)\n","for (i, batch) in enumerate(progress_bar):\n","  opt.zero_grad() # zero gradients to stop accumulation\n","\n","  # create observation\n","  x = batch.to(device)\n","  y = x + SIGMA*torch.randn_like(x)\n","\n","  # run forward pass\n","  xhat = net(y)\n","\n","  # compute loss\n","  loss = torch.mean((x-xhat)**2)\n","\n","  # run backward pass, update parameters\n","  loss.backward() # backprop on the loss wrt all the gradients\n","  opt.step() # has access to the parameters - use the optimizer to update gradients\n","\n","  loss = loss.item()\n","  avg_loss += loss\n","  progress_bar.set_postfix_str(f\"loss={loss:.3e}|avg_loss={avg_loss/(i+1):.3e}\")\n","\n","avg_loss /= i\n","\n","print(f\"AVERAGE LOSS = {avg_loss:.3e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s0X_MPB76T44"},"outputs":[],"source":["img = Image.open(ds.files[0]).convert('L')\n","imgt = tv.transforms.functional.to_tensor(img)\n","imgt.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjGgpfFN8lW9"},"outputs":[],"source":["x = imgt[None,...].to(device)\n","y = x + SIGMA*torch.randn_like(x)\n","\n","xhat = net(y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PbvM3Si9l3O"},"outputs":[],"source":["plt.imshow(x[0,0].cpu())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlwN91jZ9oAh"},"outputs":[],"source":["plt.imshow(y[0,0].cpu())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCnBd1H-9tfO"},"outputs":[],"source":["plt.imshow(xhat[0,0].cpu().detach())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRqCUB6-9yU6"},"outputs":[],"source":["def PSNR(x, xhat, maxval=1):\n","  return 20*np.log10(maxval) - 10*np.log10(torch.mean((x-xhat)**2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KyxTQh6B-y-f"},"outputs":[],"source":["PSNR(x.cpu(),xhat.cpu().detach())"]},{"cell_type":"markdown","metadata":{"id":"-Iwk0fyr_9cG"},"source":["Summary:\n","- Define your **data**\n","  - Dataset\n","  - data-augmentation (crops, flips)\n","  - dataloading (batchsize)\n","- Construct your **network**\n","  - parameters \n","  - scale (larger/smaller versions)\n","  - forward pass\n","  - initialization\n","- **Train**, validate, test\n","  - initialize an optimizer with our network's parameters\n","  - zero gradients every minibatch\n","  - validate on separate data regularly\n","  - show statistics to make debugging easier for yourself (tqdm, etc.)!"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1x8gBdLVj19TwF8udilXlEqTpRV-UF4aN","timestamp":1679861085196},{"file_id":"1QMnuGz-FktMv54b9wdFhe97sNeR9R9cr","timestamp":1679753097729}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":0}