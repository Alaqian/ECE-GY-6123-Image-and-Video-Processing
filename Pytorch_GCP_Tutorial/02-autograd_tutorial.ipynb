{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"jJem7AYl9jQY"},"source":["# Autograd: automatic differentiation\n","\n","Adopted from DS-GA 1008 Deep Learning \n","\n","The ``autograd`` package provides automatic differentiation for all operations\n","on Tensors. It is a define-by-run framework, which means that your backprop is\n","defined by how your code is run, and that every single iteration can be\n","different."]},{"cell_type":"code","metadata":{"id":"m874VhX29jQd","executionInfo":{"status":"ok","timestamp":1681649281765,"user_tz":240,"elapsed":8234,"user":{"displayName":"Alaqian Zafar","userId":"09077679626904495344"}}},"source":["import torch"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TEp1VI1u9jQe"},"source":["Create a tensor:"]},{"cell_type":"code","metadata":{"id":"mpN5gwmE9jQe","outputId":"900980fa-82c3-4de4-b1f2-221e66cf0e42","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681649282095,"user_tz":240,"elapsed":339,"user":{"displayName":"Alaqian Zafar","userId":"09077679626904495344"}}},"source":["# Create a 2x2 tensor with gradient-accumulation capabilities\n","x = torch.tensor([[1, 2], [3, 4]], requires_grad=True, dtype=torch.float32)\n","y = torch.tensor([[5, 6], [1, 2]], requires_grad=True, dtype=torch.float32)\n","\n","z = x*y\n","print(z)\n","z = z.mean()\n","print (z)\n","z.backward()\n","x.grad"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 5., 12.],\n","        [ 3.,  8.]], grad_fn=<MulBackward0>)\n","tensor(7., grad_fn=<MeanBackward0>)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([[1.2500, 1.5000],\n","        [0.2500, 0.5000]])"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"_WNliCbQ9jQf"},"source":["Do an operation on the tensor:"]},{"cell_type":"code","metadata":{"id":"0U6av11z9jQf","outputId":"25574418-2867-455f-d19b-7c49797bffe0"},"source":["# Deduct 2 from all elements\n","y = x - 2\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[-1.,  0.],\n","        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r0N6YZd59jQg"},"source":["``y`` was created as a result of an operation, so it has a ``grad_fn``.\n","\n"]},{"cell_type":"code","metadata":{"id":"EossvqCE9jQg","outputId":"ae1bebb6-9503-4b07-81d3-76458100c0cb"},"source":["print(y.grad_fn)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<SubBackward0 object at 0x7f366e61fe50>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LNqwMSlg9jQg","outputId":"f5baa7a0-b4ec-4776-edf0-25496e1cf3a6"},"source":["# What's happening here?\n","print(x.grad_fn)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lyHMmT2Q9jQg","outputId":"612bf759-c271-4cdc-9a0a-ae6a37ebf946"},"source":["# Let's dig further...\n","y.grad_fn"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<SubBackward0 at 0x7f366e61d890>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"yjYP-2KQ9jQh","outputId":"f4bc67f9-82ec-48c7-9e09-9e4ecd4d3920"},"source":["y.grad_fn.next_functions[0][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AccumulateGrad at 0x7f366e61fd10>"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"wNci3BaS9jQh","outputId":"2a390808-f182-4dd1-a3c5-ca2f7b200d94"},"source":["y.grad_fn.next_functions[0][0].variable"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [3., 4.]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"y9fbVUcR9jQh","outputId":"45e2c9ec-0ed5-4d8f-b8c2-cee3303203ea"},"source":["# Do more operations on y\n","z = y * y * 3\n","a = z.mean()  # average\n","\n","print(z)\n","print(a)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[ 3.,  0.],\n","        [ 3., 12.]], grad_fn=<MulBackward0>)\n","tensor(4.5000, grad_fn=<MeanBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Yg7lrb5-9jQi"},"source":["# Let's visualise the computational graph! (thks @szagoruyko)\n","from torchviz import make_dot"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"twLKo-w79jQi","outputId":"39c88c66-6cb4-4664-d963-20ac82affa43"},"source":["make_dot(a)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"image/svg+xml":"<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.38.0 (20140413.2041)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"106pt\" height=\"271pt\"\n viewBox=\"0.00 0.00 106.00 271.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 267)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-267 102,-267 102,4 -4,4\"/>\n<!-- 139871611916560 -->\n<g id=\"node1\" class=\"node\"><title>139871611916560</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"98,-21 0,-21 0,-0 98,-0 98,-21\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\">MeanBackward0</text>\n</g>\n<!-- 139871611916624 -->\n<g id=\"node2\" class=\"node\"><title>139871611916624</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94.5,-78 3.5,-78 3.5,-57 94.5,-57 94.5,-78\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulBackward0</text>\n</g>\n<!-- 139871611916624&#45;&gt;139871611916560 -->\n<g id=\"edge1\" class=\"edge\"><title>139871611916624&#45;&gt;139871611916560</title>\n<path fill=\"none\" stroke=\"black\" d=\"M49,-56.9197C49,-49.9083 49,-40.1442 49,-31.4652\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"52.5001,-31.3408 49,-21.3408 45.5001,-31.3409 52.5001,-31.3408\"/>\n</g>\n<!-- 139871611916752 -->\n<g id=\"node3\" class=\"node\"><title>139871611916752</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94.5,-135 3.5,-135 3.5,-114 94.5,-114 94.5,-135\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-121.4\" font-family=\"Times,serif\" font-size=\"12.00\">MulBackward0</text>\n</g>\n<!-- 139871611916752&#45;&gt;139871611916624 -->\n<g id=\"edge2\" class=\"edge\"><title>139871611916752&#45;&gt;139871611916624</title>\n<path fill=\"none\" stroke=\"black\" d=\"M49,-113.92C49,-106.908 49,-97.1442 49,-88.4652\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"52.5001,-88.3408 49,-78.3408 45.5001,-88.3409 52.5001,-88.3408\"/>\n</g>\n<!-- 139871756867728 -->\n<g id=\"node4\" class=\"node\"><title>139871756867728</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"94,-192 4,-192 4,-171 94,-171 94,-192\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-178.4\" font-family=\"Times,serif\" font-size=\"12.00\">SubBackward0</text>\n</g>\n<!-- 139871756867728&#45;&gt;139871611916752 -->\n<g id=\"edge3\" class=\"edge\"><title>139871756867728&#45;&gt;139871611916752</title>\n<path fill=\"none\" stroke=\"black\" d=\"M43.7332,-170.92C42.3546,-163.908 41.9371,-154.144 42.4804,-145.465\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"45.979,-145.684 43.6826,-135.341 39.0278,-144.858 45.979,-145.684\"/>\n</g>\n<!-- 139871756867728&#45;&gt;139871611916752 -->\n<g id=\"edge5\" class=\"edge\"><title>139871756867728&#45;&gt;139871611916752</title>\n<path fill=\"none\" stroke=\"black\" d=\"M54.2668,-170.92C55.6454,-163.908 56.0629,-154.144 55.5196,-145.465\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"58.9722,-144.858 54.3174,-135.341 52.021,-145.684 58.9722,-144.858\"/>\n</g>\n<!-- 139871756877072 -->\n<g id=\"node5\" class=\"node\"><title>139871756877072</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"76,-263 22,-263 22,-228 76,-228 76,-263\"/>\n<text text-anchor=\"middle\" x=\"49\" y=\"-235.4\" font-family=\"Times,serif\" font-size=\"12.00\"> (2, 2)</text>\n</g>\n<!-- 139871756877072&#45;&gt;139871756867728 -->\n<g id=\"edge4\" class=\"edge\"><title>139871756877072&#45;&gt;139871756867728</title>\n<path fill=\"none\" stroke=\"black\" d=\"M49,-227.885C49,-219.994 49,-210.505 49,-202.248\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"52.5001,-202.018 49,-192.018 45.5001,-202.018 52.5001,-202.018\"/>\n</g>\n</g>\n</svg>\n","text/plain":["<graphviz.dot.Digraph at 0x7f3665be1090>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"b0F622Td9jQi"},"source":["## Gradients\n","\n","Let's backprop now `out.backward()` is equivalent to doing `out.backward(torch.tensor([1.0]))`"]},{"cell_type":"code","metadata":{"id":"2kH4cWFP9jQi"},"source":["# Backprop\n","a.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sy9jP5jp9jQi"},"source":["Print gradients $\\frac{\\text{d}a}{\\text{d}x}$.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ek1tw2VK9jQj","outputId":"abe48216-3583-4276-c80a-03862fe72641"},"source":["# Compute it by hand BEFORE executing this\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[-0.2500,  1.5000],\n","        [ 1.7500,  3.5000]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nht1giSQ9jQj"},"source":["You can do many crazy things with autograd!\n","> With Great *Flexibility* Comes Great Responsibility"]},{"cell_type":"code","metadata":{"id":"bnvRnKIB9jQj","outputId":"2650796e-b1a6-4057-f497-6eca2eba80c5"},"source":["# Dynamic graphs!\n","x = torch.randn(3, requires_grad=True)\n","\n","y = x * 2\n","i = 0\n","while y.data.norm() < 1000:\n","    y = y * 2\n","    i += 1\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([-1044.7804,  -540.2243,  -263.0571], grad_fn=<MulBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q7NL45Ny9jQj","outputId":"66e66636-1827-4007-d6ae-af9e2849c9d9"},"source":["# If we don't run backward on a scalar we need to specify the grad_output\n","gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n","y.backward(gradients)\n","\n","print(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZzDnZuit9jQk","outputId":"15bd2f81-1f16-4e8f-863a-1a34d3c7d064"},"source":["# BEFORE executing this, can you tell what would you expect it to print?\n","print(i)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["9\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WZcpNqVF9jQk"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"7jNY5fMh9jQk"},"source":["# This variable decides the tensor's range below\n","n = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"raFUDng89jQk","outputId":"3d0432cd-1e27-47f3-be4a-999c8cfcb54c"},"source":["# Both x and w that allows gradient accumulation\n","x = torch.arange(1., n + 1, requires_grad=True)\n","w = torch.ones(n, requires_grad=True)\n","z = w @ x\n","z.backward()\n","print(x.grad, w.grad, sep='\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([1., 1., 1.])\n","tensor([1., 2., 3.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d1N0Ap7M9jQk","outputId":"e1589250-b948-4795-cf36-979c80b7a1f7"},"source":["# Only w that allows gradient accumulation\n","x = torch.arange(1., n + 1)\n","w = torch.ones(n, requires_grad=True)\n","z = w @ x\n","z.backward()\n","print(x.grad, w.grad, sep='\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["None\n","tensor([1., 2., 3.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FLoUyO109jQk"},"source":["x = torch.arange(1., n + 1)\n","w = torch.ones(n, requires_grad=True)\n","\n","# Regardless of what you do in this context, all torch tensors will not have gradient accumulation\n","with torch.no_grad():\n","    z = w @ x\n","\n","try:\n","    z.backward()  # PyTorch will throw an error here, since z has no grad accum.\n","except RuntimeError as e:\n","    print('RuntimeError!!! >:[')\n","    print(e)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4DYSFeu9jQl"},"source":["## More stuff\n","\n","Documentation of the automatic differentiation package is at\n","http://pytorch.org/docs/autograd."]}]}