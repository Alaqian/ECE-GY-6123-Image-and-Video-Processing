{"cells":[{"cell_type":"markdown","id":"4f55173d-ab6d-4cc6-bf38-7041a6e7c17b","metadata":{"id":"4f55173d-ab6d-4cc6-bf38-7041a6e7c17b"},"source":["# Computer Assignment 4: CNN Segmentation\n","## Alaqian Zafar - aaz7118\n","\n","## Table of Contents\n","- <a href='#p1a'>Part (a)</a>\n","- <a href='#p1b'>Part (b)</a>\n","- <a href='#p1c'>Part (c)</a>\n","- <a href='#p1d'>Part (d)</a>\n","- <a href='#p2a'>Part (e)</a>\n","- <a href='#p2b'>Part (f)</a>\n","- <a href='#p2c'>Part (g)</a>"]},{"cell_type":"code","execution_count":1,"id":"bd605007-4d11-487a-937e-870eb43f594f","metadata":{"executionInfo":{"elapsed":252,"status":"ok","timestamp":1681647629966,"user":{"displayName":"Alaqian Zafar","userId":"09077679626904495344"},"user_tz":240},"id":"bd605007-4d11-487a-937e-870eb43f594f"},"outputs":[],"source":["import os\n","import random\n","\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"id":"VXkF1WtANXVY","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20718,"status":"ok","timestamp":1681647650682,"user":{"displayName":"Alaqian Zafar","userId":"09077679626904495344"},"user_tz":240},"id":"VXkF1WtANXVY","outputId":"a48abedc-9363-44b8-91f6-6a24e827a022"},"outputs":[],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    path = '/content/drive/MyDrive/ECE-GY 6123 Image and Video Processing/Computer Assignments/CA04/archive'\n","except:\n","    path = 'archive'"]},{"cell_type":"markdown","id":"cdd71cc3-68b0-4339-82ed-5f7a15a29902","metadata":{"id":"cdd71cc3-68b0-4339-82ed-5f7a15a29902"},"source":["<a id='p1a'></a>\n","##### (a) Cut the FudanPed dataset into an 80-10-10 train-val-test split.\n","\n","[Table of Contents](#Table-of-Contents)"]},{"cell_type":"code","execution_count":33,"id":"7pLVDuBjkZp7","metadata":{"executionInfo":{"elapsed":6347,"status":"ok","timestamp":1681648012213,"user":{"displayName":"Alaqian Zafar","userId":"09077679626904495344"},"user_tz":240},"id":"7pLVDuBjkZp7"},"outputs":[{"name":"stdout","output_type":"stream","text":["archive\\PNGImages\\FudanPed00009.png archive\\PedMasks\\FudanPed00009_mask.png\n","archive\\PNGImages\\FudanPed00056.png archive\\PedMasks\\FudanPed00056_mask.png\n","archive\\PNGImages\\FudanPed00039.png archive\\PedMasks\\FudanPed00039_mask.png\n","archive\\PNGImages\\PennPed00062.png archive\\PedMasks\\PennPed00062_mask.png\n","archive\\PNGImages\\PennPed00029.png archive\\PedMasks\\PennPed00029_mask.png\n","archive\\PNGImages\\FudanPed00036.png archive\\PedMasks\\FudanPed00036_mask.png\n","archive\\PNGImages\\PennPed00090.png archive\\PedMasks\\PennPed00090_mask.png\n","archive\\PNGImages\\PennPed00045.png archive\\PedMasks\\PennPed00045_mask.png\n","archive\\PNGImages\\PennPed00001.png archive\\PedMasks\\PennPed00001_mask.png\n","archive\\PNGImages\\PennPed00041.png archive\\PedMasks\\PennPed00041_mask.png\n","archive\\PNGImages\\FudanPed00014.png archive\\PedMasks\\FudanPed00014_mask.png\n","archive\\PNGImages\\FudanPed00062.png archive\\PedMasks\\FudanPed00062_mask.png\n","archive\\PNGImages\\FudanPed00045.png archive\\PedMasks\\FudanPed00045_mask.png\n","archive\\PNGImages\\PennPed00087.png archive\\PedMasks\\PennPed00087_mask.png\n","archive\\PNGImages\\FudanPed00061.png archive\\PedMasks\\FudanPed00061_mask.png\n","archive\\PNGImages\\FudanPed00049.png archive\\PedMasks\\FudanPed00049_mask.png\n","archive\\PNGImages\\PennPed00096.png archive\\PedMasks\\PennPed00096_mask.png\n"]}],"source":["image_paths = sorted([os.path.join(path, \"PNGImages\", image) for image in os.listdir(os.path.join(path, \"PNGImages\"))])\n","mask_paths = sorted([os.path.join(path, \"PedMasks\", mask) for mask in os.listdir(os.path.join(path, \"PedMasks\"))])\n","\n","indices = list(range(len(image_paths)))\n","train_indices = random.sample(indices, k=int(len(indices)*0.8))\n","val_indices = random.sample(set(indices)-set(train_indices), k=int(len(indices)*0.1))\n","test_indices = list(set(indices)-set(train_indices)-set(val_indices))\n","\n","train_image_paths = [image_paths[i] for i in train_indices]\n","train_mask_paths = [mask_paths[i] for i in train_indices]\n","val_image_paths = [image_paths[i] for i in val_indices]\n","val_mask_paths = [mask_paths[i] for i in val_indices]\n","test_image_paths = [image_paths[i] for i in test_indices]\n","test_mask_paths = [mask_paths[i] for i in test_indices]"]},{"cell_type":"code","execution_count":33,"id":"7a75f58a","metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'preprocess_image' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[33], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(image_path)\n\u001b[0;32m      6\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m----> 7\u001b[0m images\u001b[39m.\u001b[39mappend(preprocess_image(image))\n\u001b[0;32m      9\u001b[0m mask_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(path, \u001b[39m\"\u001b[39m\u001b[39mPedMasks\u001b[39m\u001b[39m\"\u001b[39m, mask_path)\n\u001b[0;32m     10\u001b[0m mask \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(mask_path, \u001b[39m0\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'preprocess_image' is not defined"]}],"source":["images = []\n","masks = []\n","for i, (image_path, mask_path) in enumerate(zip(image_paths, mask_paths)):\n","    image_path = os.path.join(path, \"PNGImages\", image_path)\n","    image = cv2.imread(image_path)\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    images.append(preprocess_image(image))\n","    \n","    mask_path = os.path.join(path, \"PedMasks\", mask_path)\n","    mask = cv2.imread(mask_path, 0)\n","    masks.append(preprocess_image(mask))"]},{"cell_type":"markdown","id":"33284490-af37-49c1-a5bd-c7d48472fcfc","metadata":{"id":"33284490-af37-49c1-a5bd-c7d48472fcfc","tags":[]},"source":["<a id='p1b'></a>\n","##### (b) Apply data augmentation to your dataset during training and show an example of your data augmentation in your report.\n","\n","[Table of Contents](#Table-of-Contents)"]},{"cell_type":"code","execution_count":null,"id":"GxzoyQCWOS2V","metadata":{"executionInfo":{"elapsed":190,"status":"ok","timestamp":1681648088672,"user":{"displayName":"Alaqian Zafar","userId":"09077679626904495344"},"user_tz":240},"id":"GxzoyQCWOS2V"},"outputs":[],"source":["images = [torch.from_numpy(item).float() for item in images]"]},{"cell_type":"code","execution_count":null,"id":"yRZlE58BLylM","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1681647650682,"user":{"displayName":"Alaqian Zafar","userId":"09077679626904495344"},"user_tz":240},"id":"yRZlE58BLylM"},"outputs":[],"source":["def preprocess_image(image):\n","    # pad the image with zeros to make it square\n","    image = cv2.copyMakeBorder(\n","        image,\n","        0, \n","        max(image.shape) - image.shape[0], \n","        0, \n","        max(image.shape) - image.shape[1],\n","        cv2.BORDER_CONSTANT, \n","        None, \n","        value = 0)\n","    \n","    # resize the image to 64 x 64\n","    image = cv2.resize(image, (128,128))\n","\n","    # reshape the image to have channel first\n","    #image = np.transpose(image, (2, 0, 1))\n","    return image"]},{"cell_type":"markdown","id":"b1d3219f-8a19-461e-8147-acdc6e77deda","metadata":{"id":"b1d3219f-8a19-461e-8147-acdc6e77deda","tags":[]},"source":["<a id='p1c'></a>\n","##### (c) Implement and train a CNN for binary segmentation on your train split. Describe your network architecture2, loss function, and any training hyper-parameters. You may implement any architecture you'd like, **but the implementation must be your own code.**\n","\n","[Table of Contents](#Table-of-Contents)"]},{"cell_type":"markdown","id":"6e1f0531-2aa0-4daf-9f8a-9315f9257e79","metadata":{"id":"6e1f0531-2aa0-4daf-9f8a-9315f9257e79"},"source":["<a id='p1d'></a>\n","##### (d) Report training loss, validation loss, and validation DICE curves. Comment on any overfitting or underfitting observed.\n","\n","[Table of Contents](#Table-of-Contents)"]},{"cell_type":"markdown","id":"f2b0e596-7345-4eaa-8e9d-de9801d5b8fa","metadata":{"id":"f2b0e596-7345-4eaa-8e9d-de9801d5b8fa"},"source":["<a id='p2a'></a>\n","##### (e) Report the average dice score over your test-set. **You should be able to achieve a score of around 0.7 or better**.\n","\n","[Table of Contents](#Table-of-Contents)"]},{"cell_type":"markdown","id":"88d56e64-544c-439f-a474-6b24f5e2d93f","metadata":{"id":"88d56e64-544c-439f-a474-6b24f5e2d93f","tags":[]},"source":["<a id='p2b'></a>\n","##### (f) Show at least 3 example segmentations (i.e. show the RGB image, mask, and RGB image X mask for 3 samples) from your training data and 3 from your testing data. Comment on the generalization capabilities of your trained network.\n","\n","[Table of Contents](#Table-of-Contents)"]},{"cell_type":"markdown","id":"e3de73fb-1143-4c0f-acbe-642cbdfb0b96","metadata":{"id":"e3de73fb-1143-4c0f-acbe-642cbdfb0b96","tags":[]},"source":["<a id='p2c'></a>\n","##### (g) Show at least 1 example segmentation on an input image **<ins>not</ins> from the FudanPed dataset**. Again, comment on the generalization capabilities of your network with respect to this \"out-of-distribution\" image.\n","\n","[Table of Contents](#Table-of-Contents)"]},{"cell_type":"code","execution_count":null,"id":"f6733ef6","metadata":{"tags":["run_all"]},"outputs":[{"name":"stderr","output_type":"stream","text":["[NbConvertApp] WARNING | Config option `kernel_spec_manager_class` not recognized by `NbConvertApp`.\n","[NbConvertApp] Converting notebook aaz7118-CA04.ipynb to html\n","[NbConvertApp] Writing 287941 bytes to aaz7118-CA04.html\n"]}],"source":["# Create a README.md from this notebook\n","!jupyter nbconvert --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags run_all aaz7118-CA04.ipynb --to html --template classic"]}],"metadata":{"colab":{"collapsed_sections":["33284490-af37-49c1-a5bd-c7d48472fcfc","b1d3219f-8a19-461e-8147-acdc6e77deda","6e1f0531-2aa0-4daf-9f8a-9315f9257e79","f2b0e596-7345-4eaa-8e9d-de9801d5b8fa","88d56e64-544c-439f-a474-6b24f5e2d93f","e3de73fb-1143-4c0f-acbe-642cbdfb0b96"],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":5}
